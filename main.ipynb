{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Check the [pdf report](https://github.com/DavideEspositoPelella/Anomaly_Detection-GAN_Based.git/Report) or the [GitHub repository](https://github.com/DavideEspositoPelella/Anomaly_Detection-GAN_Based.git).\n",
    "\n",
    "The following Colab notebook represent the implementation of the Deep Learning project _**GAN-based Anomaly Detection in Imbalance Problems**_. \n",
    "\n",
    "<a href=\"https://colab.research.google.com/drive/1wHPGH00jiXZUCCsaCI6dreoZdNqTyijK?usp=share_link\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"google colab logo\" height=30px></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "Importing all the libraries and dependencies needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --quiet \"pytorch-lightning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import torchvision.transforms as T\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import random\n",
    "\n",
    "print(\"Lightning version:\", pl.__version__)\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"Python version:\", platform.python_version())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly rotate the images by 90, 180, or 270 degrees\n",
    "class Rotation3:\n",
    "    def __call__(self, img):\n",
    "        angle = random.choice([0, 90, 180, 270])\n",
    "        img = T.functional.rotate(img, angle)\n",
    "        return img\n",
    "\n",
    "# DataModule\n",
    "class AnomalyDetectionDataModule_MNIST(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size, subsampling, subsamples, normal_class, data_dir=\"./\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.normal_class = normal_class\n",
    "        self.subsampling = subsampling\n",
    "        self.subsamples = subsamples\n",
    "\n",
    "        self.train_transform = T.Compose([\n",
    "            T.Resize(32),\n",
    "            T.RandomVerticalFlip(),\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.RandomCrop(30),\n",
    "            T.Resize(32),\n",
    "            T.Grayscale(1),  # Use 1 channel (grayscale)\n",
    "            T.RandomApply([Rotation3()], p=1),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.5], [0.5])\n",
    "        ])\n",
    "        self.test_transform = T.Compose([\n",
    "            T.Resize(32),\n",
    "            T.Grayscale(1),  # Use 1 channel (grayscale)\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.5], [0.5])\n",
    "        ])\n",
    "\n",
    "        self.data_dir = data_dir\n",
    "        self.dataset_classes = 10\n",
    "        self.num_classes = 2  # Normal and Anomaly\n",
    "        self.batch_size = batch_size\n",
    "        self.num_clusters = self.dataset_classes - 1\n",
    "        self.subsamples = subsamples\n",
    "        self.subsampling = subsampling\n",
    "\n",
    "    # K-means based sampling of anomal classes\n",
    "    def kmeans_sampling(self, dataset, class_idx, num_samples):\n",
    "        indices = [i for i in range(len(dataset)) if dataset.targets[i] != class_idx]\n",
    "        class_data = torch.stack([dataset[i][0] for i in indices])\n",
    "        kmeans = KMeans(n_clusters=self.num_clusters, random_state=0, n_init=9) #n_init = 9\n",
    "        kmeans.fit(class_data.view(class_data.size(0), -1).numpy())\n",
    "        cluster_assignments = kmeans.predict(class_data.view(class_data.size(0), -1).numpy())\n",
    "        sampled_indices = []\n",
    "        for cluster_idx in range(self.num_clusters):\n",
    "            cluster_indices = np.where(cluster_assignments == cluster_idx)[0]\n",
    "            num_samples_from_cluster = min(num_samples, len(cluster_indices))\n",
    "            sampled_indices.extend(np.random.choice(cluster_indices, num_samples_from_cluster, replace=False))\n",
    "        return [indices[idx] for idx in sampled_indices]\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        FashionMNIST(root=self.data_dir, train=True, download=True)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "\n",
    "        if stage == 'fit' or stage is None: # Setup dataset for training phase\n",
    "            \n",
    "            all_data = []\n",
    "            all_labels = []\n",
    "            normal_data  = []\n",
    "            anomal_data = []\n",
    "            normal_class = self.normal_class\n",
    "\n",
    "            # Load dataset\n",
    "            train_dataset = FashionMNIST(root=self.data_dir, train=True, transform=self.train_transform)\n",
    "\n",
    "            # Use 1 class as 'normal' and the other as 'anomal'\n",
    "            normal_indices = [i for i in range(len(train_dataset)) if train_dataset.targets[i] == normal_class]\n",
    "            anomal_indices = self.kmeans_sampling(train_dataset, normal_class, num_samples=len(normal_indices))\n",
    "            \n",
    "            # Use only a subset of dataset\n",
    "            if self.subsampling == True:\n",
    "                normal_indices = normal_indices[:self.subsamples]\n",
    "                anomal_indices = anomal_indices[:self.subsamples]\n",
    "\n",
    "            normal_labels = torch.ones(len(normal_indices), dtype=torch.float32)\n",
    "            anomal_labels = torch.zeros(len(anomal_indices), dtype=torch.float32)\n",
    "\n",
    "            normal_data.extend([train_dataset[i][0] for i in normal_indices])\n",
    "            anomal_data.extend([train_dataset[i][0] for i in anomal_indices])\n",
    "\n",
    "            all_data.extend(normal_data)\n",
    "            all_data.extend(anomal_data)\n",
    "            all_labels.extend(normal_labels)\n",
    "            all_labels.extend(anomal_labels)\n",
    "\n",
    "            all_data = torch.stack(all_data)\n",
    "            all_labels = torch.tensor(all_labels)\n",
    "\n",
    "            # Build the new dataset following requirements\n",
    "            self.train_dataset = torch.utils.data.TensorDataset(all_data, all_labels)\n",
    "\n",
    "        if stage == 'test' or stage is None: # Setup dataset for training phase\n",
    "            \n",
    "            all_data = []\n",
    "            all_labels = []\n",
    "            normal_data  = []\n",
    "            anomal_data = []\n",
    "            normal_class = self.normal_class\n",
    "\n",
    "            # Load test dataset\n",
    "            test_dataset = FashionMNIST(root=self.data_dir, train=False, transform=self.test_transform)\n",
    "\n",
    "            normal_indices = [i for i in range(len(test_dataset)) if test_dataset.targets[i] == normal_class]\n",
    "            anomal_indices = [i for i in range(len(test_dataset)) if test_dataset.targets[i] != normal_class]\n",
    "            \n",
    "            if self.subsampling == True:\n",
    "                normal_indices = normal_indices[:self.subsamples]\n",
    "                anomal_indices = anomal_indices[:self.subsamples]\n",
    "\n",
    "            normal_labels = torch.ones(len(normal_indices), dtype=torch.float32)\n",
    "            anomal_labels = torch.zeros(len(anomal_indices), dtype=torch.float32)\n",
    "\n",
    "            normal_data.extend([test_dataset[i][0] for i in normal_indices])\n",
    "            anomal_data.extend([test_dataset[i][0] for i in anomal_indices])\n",
    "\n",
    "            all_data.extend(normal_data)\n",
    "            all_data.extend(anomal_data)\n",
    "            all_labels.extend(normal_labels)\n",
    "            all_labels.extend(anomal_labels)\n",
    "\n",
    "            all_data = torch.stack(all_data)\n",
    "            all_labels = torch.tensor(all_labels)\n",
    "\n",
    "            # Build the new dataset \n",
    "            self.test_dataset = torch.utils.data.TensorDataset(all_data, all_labels)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.anomaly_data, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "Utilities and models implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN-Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ENCODER(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=4, stride=2, padding=1)  #out 16x16\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1) #out 8x8\n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1) #out 4x4\n",
    "        self.conv4 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=4, stride=2, padding=1) #out 2x2\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        return x\n",
    "\n",
    "class DECODER(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()   \n",
    "\n",
    "        self.tr_conv1 = nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=4, stride=2, padding=1) #out 4x4\n",
    "        self.tr_conv2 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=4, stride=2, padding=1) #out 8x8\n",
    "        self.tr_conv3 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=4, stride=2, padding=1) #out 16x16\n",
    "        self.tr_conv4 = nn.ConvTranspose2d(in_channels=64, out_channels=1, kernel_size=4, stride=2, padding=1) #out 32x32\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.tr_conv1(x))\n",
    "        x = F.relu(self.tr_conv2(x))\n",
    "        x = F.relu(self.tr_conv3(x))\n",
    "        x = F.tanh(self.tr_conv4(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class GENERATOR(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = ENCODER()\n",
    "        self.decoder = DECODER()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DISCRIMINATOR(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=4, stride=2, padding=1)  #out 32x16x16\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1) #out 64x8x8\n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=64, kernel_size=4, stride=2, padding=1) #out 128x4x4\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=4, stride=1, padding=1) #out 32x3x3\n",
    "        self.conv5 = nn.Conv2d(in_channels=32, out_channels=1, kernel_size=4, stride=1, padding=1) #out 1x2x2\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.relu(self.conv5(x))\n",
    "        return x\n",
    "    \n",
    "\n",
    "class GAN_SOTA(pl.LightningModule):\n",
    "    def __init__(self, latent_dim=100, lr=0.0002, display=False, b1=0.5, b2=0.999):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.display = display\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        self.G = GENERATOR()\n",
    "        self.Enc = self.G.encoder\n",
    "        self.D_norm = DISCRIMINATOR()\n",
    "        self.D_anom = DISCRIMINATOR()\n",
    "\n",
    "        self.automatic_optimization = False\n",
    "        self.test_step_outputs = [[], []]  # two dataloaders\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.G(x)\n",
    "    \n",
    "    def print_generator_parameters(self):\n",
    "        for name, param in self.G.named_parameters():\n",
    "            print(f\"Generator Parameter Name: {name}\")\n",
    "            print(f\"Parameter Value:\\n{param}\")\n",
    "\n",
    "    def patch_loss(self, X, generated_X, n=3):\n",
    "        # Calculate the L1 loss for each patch\n",
    "        patch_errors = torch.abs(X - generated_X)\n",
    "        patch_errors_reshaped = patch_errors.view(patch_errors.size(0), patch_errors.size(1), -1)\n",
    "        mean_patch_errors = patch_errors_reshaped.mean(dim=-1)\n",
    "        # Select the top n patches\n",
    "        sorted_patch_indices = torch.argsort(mean_patch_errors, dim=-1, descending=True)\n",
    "        top_patch_indices = sorted_patch_indices[:, :n]\n",
    "        # Calculate the average of the top n patch errors\n",
    "        top_patch_errors = torch.gather(mean_patch_errors, dim=-1, index=top_patch_indices)\n",
    "        avg_top_patch_errors = top_patch_errors.mean()\n",
    "        return avg_top_patch_errors\n",
    "    \n",
    "    def show_batch_image(self, batch):\n",
    "        X, Y = batch\n",
    "        image = X[0].detach().cpu().numpy()\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.title(\"Label: {}\".format(Y[0]))\n",
    "        plt.imshow(image[0], cmap='gray')  # Assuming the input image is single-channel (gray)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    #The generator is trained to output 1 from normal data and 0 from anomaly data\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        g_opt, d_norm_opt, d_anom_opt = self.optimizers()\n",
    "\n",
    "        X, Y = batch\n",
    "        batch_size = X.shape[0]\n",
    "\n",
    "        real_label = torch.ones((batch_size, 1, 2, 2), device=self.device)\n",
    "        fake_label = torch.zeros((batch_size, 1, 2, 2), device=self.device)\n",
    "\n",
    "        errD_anomal = 0\n",
    "        errD_normal = 0\n",
    "\n",
    "        if (Y.squeeze() == 1):\n",
    "            generated_X = self.G(X)\n",
    "            discriminated_X = self.D_norm(X)\n",
    "            discriminated_G = self.D_norm(generated_X)\n",
    "            encoded_X = self.G.encoder(X)\n",
    "\n",
    "            #------------------------#\n",
    "            # Optimize Discriminator #\n",
    "            d_norm_opt.zero_grad()\n",
    "\n",
    "            # NORM ADV LOSS\n",
    "            Norm_adv_loss = (((discriminated_X - real_label)**2) + ((discriminated_G - fake_label)**2))\n",
    "\n",
    "            errD_normal = (Norm_adv_loss.mean())\n",
    "\n",
    "            errD = errD_normal\n",
    "            self.manual_backward(errD_normal.mean())\n",
    "            d_norm_opt.step()\n",
    "\n",
    "            #--------------------#\n",
    "            # Optimize Generator #\n",
    "            g_opt.zero_grad()\n",
    "\n",
    "            generated_X = self.G(X)\n",
    "            discriminated_X = self.D_norm(X)\n",
    "            discriminated_G = self.D_norm(generated_X)\n",
    "            encoded_X = self.G.encoder(X)\n",
    "\n",
    "            # L1 RECONSTRUCTION ERROR\n",
    "            l1_loss = F.l1_loss(X, generated_X)\n",
    "\n",
    "            # PATCH L1 LOSS\n",
    "            patch_loss = self.patch_loss(X, generated_X)\n",
    "\n",
    "            # LATENT VECTOR LOSS\n",
    "            latent_loss = F.l1_loss(encoded_X, self.G.encoder(generated_X))\n",
    "\n",
    "            # ADVERSARIAL LOSS\n",
    "            norm_adv_loss = ((discriminated_G - real_label)**2)\n",
    "\n",
    "            errG_normal = patch_loss*(1.5) + (norm_adv_loss)*(0.5) + (latent_loss)*(0.5) + (l1_loss)*(1.5)\n",
    "            errG = errG_normal.mean()\n",
    "            self.manual_backward(errG_normal.mean())\n",
    "            g_opt.step()\n",
    "\n",
    "            self.log_dict({\"g_loss\": torch.tensor(errG).mean(), \"d_normal_loss\": torch.tensor(errD).mean()}, prog_bar=True)\n",
    "\n",
    "        else:\n",
    "            generated_X = self.G(X)\n",
    "            discriminated_X = self.D_anom(X)\n",
    "            discriminated_G = self.D_anom(generated_X)\n",
    "        \n",
    "            # Optimize Discriminator #\n",
    "            d_anom_opt.zero_grad()\n",
    "\n",
    "            # ANOM ADV LOSS\n",
    "            Anom_adv_loss = ((((discriminated_X - real_label)**2) + ((discriminated_G - fake_label)**2)))\n",
    "            \n",
    "            errD_anomal = (Anom_adv_loss.mean())\n",
    "\n",
    "            errD = errD_anomal\n",
    "            self.manual_backward(errD_anomal.mean())\n",
    "            d_anom_opt.step()\n",
    "\n",
    "            # Optimize Generator #\n",
    "            g_opt.zero_grad()\n",
    "            generated_X = self.G(X)\n",
    "            discriminated_X = self.D_anom(X)\n",
    "            discriminated_G = self.D_anom(generated_X)\n",
    "            encoded_X = self.G.encoder(X)\n",
    "\n",
    "            # ANOM ADVERS LOSS\n",
    "            anom_adv_loss = ((discriminated_G - fake_label)**2)\n",
    "\n",
    "            # ABC LOSS\n",
    "            abc_loss = -torch.log(1 - torch.exp(-F.l1_loss(generated_X, X)))\n",
    "           \n",
    "            errG_anomal = anom_adv_loss*(1) + (abc_loss)*(0.5)\n",
    "            errG = errG_anomal.mean()\n",
    "            self.manual_backward(errG_anomal.mean())\n",
    "            g_opt.step()\n",
    "\n",
    "            self.log_dict({\"g_loss\": errG, \"d_anomal_loss\": errD}, prog_bar=True)\n",
    "            \n",
    "        return {\"g_loss\": errG, \"d_norm_loss\": errD_normal, \"d_anom_loss\": errD_anomal}\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        X, Y = batch\n",
    "\n",
    "        generated_X = self.G(X)\n",
    "        \n",
    "        # Calculate L1 reconstruction error instead of MSE\n",
    "        reconstruction_error = F.mse_loss(generated_X, X, reduction='none')\n",
    "        reconstruction_error = reconstruction_error.view(reconstruction_error.size(0), -1).mean(dim=1)  # Calculate mean error over pixels\n",
    "        self.test_step_outputs[0].append(reconstruction_error)\n",
    "        self.test_step_outputs[1].append(Y)\n",
    "\n",
    "        if self.display:\n",
    "            print(\"generated_X:\", generated_X)\n",
    "            print(\"reconstruction_error:\", reconstruction_error)\n",
    "            self.show_batch_image(batch)\n",
    "        \n",
    "        return {\"reconstruction_error\": reconstruction_error, \"true_labels\": Y}\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        all_reconstruction_errors = torch.cat(self.test_step_outputs[0])\n",
    "        all_true_labels = torch.cat(self.test_step_outputs[1])\n",
    "\n",
    "        anomaly_threshold = 0.5\n",
    "        predicted_labels = (all_reconstruction_errors < anomaly_threshold).float().cpu().numpy()\n",
    "        true_labels = all_true_labels.squeeze().float().cpu().numpy()\n",
    "        \n",
    "        # Calculate AUROC\n",
    "        auroc = roc_auc_score(true_labels, predicted_labels)\n",
    "\n",
    "        self.log(\"auroc\", (auroc), prog_bar=True)\n",
    "\n",
    "        # Clear the test step outputs after processing them\n",
    "        self.test_step_outputs = [[], []]\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.log('g_loss_epoch', self.trainer.logged_metrics['g_loss'].mean(), prog_bar=True)\n",
    "        self.log('d_anomal_loss_epoch', self.trainer.logged_metrics['d_anomal_loss'].mean(), prog_bar=True)\n",
    "        self.log('d_normal_loss_epoch', self.trainer.logged_metrics['d_normal_loss'].mean(), prog_bar=True)\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr = self.hparams.lr\n",
    "        b1 = self.hparams.b1\n",
    "        b2 = self.hparams.b2\n",
    "\n",
    "        opt_g = torch.optim.Adam(self.G.parameters(), lr=self.lr, betas=(b1, b2))\n",
    "        opt_d_norm = torch.optim.Adam(self.D_norm.parameters(), lr=self.lr, betas=(b1, b2))\n",
    "        opt_d_anom = torch.optim.Adam(self.D_anom.parameters(), lr=self.lr, betas=(b1, b2))\n",
    "        return opt_g, opt_d_norm, opt_d_anom\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Guessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomGuessing:\n",
    "    def __init__(self):\n",
    "        self.predictions = []\n",
    "        self.true_labels = []\n",
    "\n",
    "    def predict(self, dataloader):\n",
    "        self.predictions = []\n",
    "        self.true_labels = []\n",
    "\n",
    "        for batch in dataloader:\n",
    "            inputs, labels = batch\n",
    "            random_scores = torch.rand(inputs.size(0))\n",
    "            \n",
    "            self.predictions.append(random_scores)\n",
    "            self.true_labels.append(labels)\n",
    "\n",
    "        self.predictions = torch.cat(self.predictions).cpu().numpy()\n",
    "        self.true_labels = torch.cat(self.true_labels).cpu().numpy()\n",
    "\n",
    "        return self.predictions\n",
    "\n",
    "    def compute_auroc(self):\n",
    "        # Calculate AUROC\n",
    "        auroc = roc_auc_score(self.true_labels, self.predictions)\n",
    "        return auroc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Linear(784, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 784),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encode = self.enc(x)\n",
    "        decode = self.dec(encode)\n",
    "        return decode\n",
    "\n",
    "class AE_based(pl.LightningModule):\n",
    "    def __init__(self, latent_dim=100, lr=0.0002, b1=0.5, b2=0.999):\n",
    "        super(AE_based, self).__init__()\n",
    "        self.lr = lr\n",
    "        self.save_hyperparameters()\n",
    "        self.AE = Autoencoder()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.AE(x)\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, Y = batch\n",
    "        batch_size = X.shape[0]\n",
    "\n",
    "        generated_X = self.AE(X)\n",
    "\n",
    "        loss = F.mse_loss(X, generated_X).mean()\n",
    "\n",
    "        self.log_dict({\"train loss\": loss}, prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        X, Y = batch\n",
    "        batch_size = X.shape[0]\n",
    "\n",
    "        generated_X = self.AE(X)\n",
    "\n",
    "        loss = F.mse_loss(X, generated_X).mean()\n",
    "\n",
    "        self.log_dict({\"test loss\": loss}, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr = self.lr\n",
    "        b1 = self.b1\n",
    "        b2 = self.b2\n",
    "        opt = torch.optim.AdamW(self.parameters(), lr=lr, betas=(b1, b2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "Here follow the training sections of the models. In order to properly train a model, execute in the following order the respective cells:\n",
    "1) Import\n",
    "2) Dataset \n",
    "3) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities\n",
    "checkpoint_path = './GAN/checkpoints'\n",
    "os.makedirs(checkpoint_path, exist_ok=True)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=checkpoint_path,  \n",
    "    filename=\"GAN_{epoch}_ckpt\",  \n",
    "    monitor='auc',  \n",
    "    mode='min',     \n",
    "    save_last=True, \n",
    "    save_top_k=-1,  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 1\n",
    "epochs = 15\n",
    "learning_rate = 0.0001\n",
    "subsampling = True\n",
    "subsamples = 1000\n",
    "accelerator_enabled = True\n",
    "display = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataset\n",
    "data_module = AnomalyDetectionDataModule(batch_size=BATCH_SIZE, \n",
    "                                         subsampling=subsampling, \n",
    "                                         subsamples=subsamples,\n",
    "                                         normal_class=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models and trainers\n",
    "if torch.cuda.is_available() and accelerator_enabled: \n",
    "  print(\"GPU\")\n",
    "  model = GAN_SOTA(BATCH_SIZE, lr=learning_rate, display=display).to(\"cuda\")\n",
    "  trainer = pl.Trainer(accelerator=\"cuda\",\n",
    "                       max_epochs=epochs,\n",
    "                       callbacks=[TQDMProgressBar(),\n",
    "                                  checkpoint_callback]\n",
    "                       )\n",
    "else : \n",
    "  print(\"CPU\")\n",
    "  model = GAN_SOTA(BATCH_SIZE, lr=learning_rate, display=display).to(\"cpu\")\n",
    "  trainer = pl.Trainer(accelerator=\"cpu\",\n",
    "                       max_epochs=epochs, \n",
    "                       callbacks=[TQDMProgressBar(),\n",
    "                                  checkpoint_callback]\n",
    "                       )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and save\n",
    "trainer.fit(model, data_module)\n",
    "trainer.save_checkpoint(\"/GAN/checkpoints/GAN.ckpt\")\n",
    "torch.save(model.state_dict(), \"./GAN/GAN_{samples}samples_{epochs}ep\")\n",
    "\n",
    "# Testing\n",
    "test_results = trainer.test(model, data_module)\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random guessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the AnomalyDetectionDataModule\n",
    "data_module = AnomalyDetectionDataModule(batch_size=64, subsampling=True, subsamples=1000)\n",
    "\n",
    "# Setup the data module (prepare datasets)\n",
    "data_module.setup()\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = data_module.train_dataloader()\n",
    "test_dataloader = data_module.test_dataloader()\n",
    "\n",
    "random_guesser = RandomGuessing()\n",
    "test_predictions = random_guesser.predict(test_dataloader)\n",
    "print(test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities\n",
    "checkpoint_path = './AE/checkpoints'\n",
    "os.makedirs(checkpoint_path, exist_ok=True)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=checkpoint_path,  \n",
    "    filename=\"AE_{epoch}_ckpt\",  \n",
    "    monitor='auc',  \n",
    "    mode='min',     \n",
    "    save_last=True, \n",
    "    save_top_k=-1,  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 1\n",
    "epochs = 15\n",
    "learning_rate = 0.0001\n",
    "subsampling = True\n",
    "subsamples = 1000\n",
    "accelerator_enabled = True\n",
    "display = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataset\n",
    "data_module = AnomalyDetectionDataModule(batch_size=BATCH_SIZE, \n",
    "                                         subsampling=subsampling, \n",
    "                                         subsamples=subsamples,\n",
    "                                         normal_class=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and trainer\n",
    "\n",
    "if torch.cuda.is_available() and accelerator_enabled: \n",
    "  print(\"GPU\")\n",
    "  model = AE_based(BATCH_SIZE, lr=learning_rate, display=display).to(\"cuda\")\n",
    "  trainer = pl.Trainer(accelerator=\"cuda\",\n",
    "                       max_epochs=epochs,\n",
    "                       callbacks=[TQDMProgressBar(),\n",
    "                                  checkpoint_callback]\n",
    "                       )\n",
    "else : \n",
    "  print(\"CPU\")\n",
    "  model = AE_based(BATCH_SIZE, lr=learning_rate, display=display).to(\"cpu\")\n",
    "  trainer = pl.Trainer(accelerator=\"cpu\",\n",
    "                       max_epochs=epochs, \n",
    "                       callbacks=[TQDMProgressBar(),\n",
    "                                  checkpoint_callback]\n",
    "                       )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer\n",
    "trainer.fit(model, datamodule=data_module)\n",
    "trainer.save_checkpoint(\"/AE/checkpoints/GAN.ckpt\")\n",
    "torch.save(model.state_dict(), \"./AE/AE_{samples}samples_{epochs}ep\")\n",
    "\n",
    "# Testing\n",
    "test_results = trainer.test(model, data_module)\n",
    "print(test_results)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
