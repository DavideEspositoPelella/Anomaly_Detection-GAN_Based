{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsvvtN3k8hpf"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmtZF2WB8hpl",
        "outputId": "f43d6309-fa98-4672-dacc-3de0e8a2eed9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m725.0/725.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.6/731.6 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "! pip install --quiet \"pytorch-lightning\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ir8IfAjC8hpo"
      },
      "source": [
        "### Import dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aHP7e4Y8hpp",
        "outputId": "bd91a22f-0af2-4732-d5a4-7335afc25ad0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lightning version: 2.0.7\n",
            "PyTorch version: 2.0.1+cu118\n",
            "Python version: 3.10.12\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pytorch_lightning as pl\n",
        "import torchvision.datasets\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torchvision.datasets import FashionMNIST\n",
        "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
        "from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
        "from pytorch_lightning.loggers.csv_logs import CSVLogger\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "import random\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import platform\n",
        "\n",
        "print(\"Lightning version:\", pl.__version__)\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"Python version:\", platform.python_version())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCJaWt2k8hpp"
      },
      "source": [
        "### Reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "HdZ8vgpd8hpq",
        "outputId": "8602d54a-d4d7-4ea8-bfb5-bf6e5d4029b9"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-9322ec2f03ec>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'pl' is not defined"
          ]
        }
      ],
      "source": [
        "pl.seed_everything(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fB4i4aWhmrzT"
      },
      "source": [
        "-------------------------------------------\n",
        "### Manual Dataset Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gGTpzYpmtdg",
        "outputId": "f2917e6d-35f2-43f7-c7b0-312bbbc9b658"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainset: 60000\n",
            "Testset: 10000\n"
          ]
        }
      ],
      "source": [
        "class Rotation3:\n",
        "    def __call__(self, img):\n",
        "        # Randomly rotate the images by 90, 180, or 270 degrees\n",
        "        angle = random.choice([0, 90, 180, 270])\n",
        "        img = T.functional.rotate(img, angle)\n",
        "        return img\n",
        "\n",
        "samples = 16\n",
        "data_transforms = {\n",
        "    'train': T.Compose([T.Resize((32, 32)),                  # resizing\n",
        "                        T.RandomVerticalFlip(),              # Flip images vertically with a certain probability\n",
        "                        T.RandomHorizontalFlip(),            # Flip images horizontally with a certain probability\n",
        "                        T.RandomCrop(30),                    # Randomly crop the images by 0-2 pixels from their boundary\n",
        "                        T.Grayscale(1),                      # 1 Grayscale -> 3 RGB channel\n",
        "                        T.Resize((32, 32)),                  # resized after crop\n",
        "                        T.RandomApply([Rotation3()], p=1),   #randomly rotate\n",
        "                        T.ToTensor(),\n",
        "                        T.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "    ]),\n",
        "    'test': T.Compose([T.Resize(32),\n",
        "                       T.Grayscale(1),\n",
        "                       T.ToTensor(),\n",
        "                       T.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "    ]),\n",
        "    'display': T.Compose([T.Resize(32),\n",
        "                          T.Grayscale(1),\n",
        "                          T.ToTensor(),\n",
        "                          T.Normalize([0.5], [0.5])\n",
        "    ])\n",
        "}\n",
        "\n",
        "#Load data train and test\n",
        "trainset = FashionMNIST(root='./data', train=True,\n",
        "                        download=True, transform=data_transforms['train'])\n",
        "print('Trainset:', len(trainset))\n",
        "\n",
        "displayset = FashionMNIST(root='./data', train=True,\n",
        "                        download=True, transform=data_transforms['display'])\n",
        "\n",
        "testset = FashionMNIST(root='./data', train=False,\n",
        "                       download=True,transform=data_transforms['test'])\n",
        "print('Testset:', len(testset))\n",
        "\n",
        "#Prepare validation\n",
        "indices = list(range(len(trainset)))\n",
        "#np.random.shuffle(devices)\n",
        "\n",
        "split = int(np.floor(0.2 * len(trainset)))\n",
        "train_sample = SubsetRandomSampler(indices[:split])\n",
        "valid_sample = SubsetRandomSampler(indices[split:])\n",
        "\n",
        "#Data loader\n",
        "trainloader = DataLoader(trainset, batch_size = samples, shuffle = False, num_workers = 2)\n",
        "testloader = DataLoader(testset, batch_size = samples, shuffle = True, num_workers = 2)\n",
        "valloader = DataLoader(trainset, batch_size = samples, shuffle = False, num_workers = 2)\n",
        "displayloader = DataLoader(displayset, batch_size = samples, shuffle = False, num_workers = 2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlHgDM6m3iKx"
      },
      "source": [
        "### Dataset Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rg0FitTL34K4"
      },
      "outputs": [],
      "source": [
        "# Define a dictionary mapping class indices to class names\n",
        "class_names = {\n",
        "    0: \"T-shirt/top\",\n",
        "    1: \"Trouser\",\n",
        "    2: \"Pullover\",\n",
        "    3: \"Dress\",\n",
        "    4: \"Coat\",\n",
        "    5: \"Sandal\",\n",
        "    6: \"Shirt\",\n",
        "    7: \"Sneaker\",\n",
        "    8: \"Bag\",\n",
        "    9: \"Ankle boot\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 737
        },
        "id": "cLNuXlZjzrVP",
        "outputId": "e7efa615-9bf8-405f-8a83-1572971a265c"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-bcb23c40990f>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Iterate through the dataset and count samples for each class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mclass_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1344\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1345\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1371\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1372\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torchvision/datasets/mnist.py\", line 145, in __getitem__\n    img = self.transform(img)\n  File \"/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\", line 95, in __call__\n    img = t(img)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\", line 277, in forward\n    return F.normalize(tensor, self.mean, self.std, self.inplace)\n  File \"/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py\", line 363, in normalize\n    return F_t.normalize(tensor, mean=mean, std=std, inplace=inplace)\n  File \"/usr/local/lib/python3.10/dist-packages/torchvision/transforms/_functional_tensor.py\", line 928, in normalize\n    return tensor.sub_(mean).div_(std)\nRuntimeError: output with shape [1, 32, 32] doesn't match the broadcast shape [3, 32, 32]\n"
          ]
        }
      ],
      "source": [
        "# Create a dictionary to store class counts\n",
        "class_counts = defaultdict(int)\n",
        "\n",
        "# Iterate through the dataset and count samples for each class\n",
        "for _, labels in trainloader:\n",
        "    for label in labels:\n",
        "        class_counts[label.item()] += 1\n",
        "\n",
        "# Print the class counts along with class names in an ordered way\n",
        "for class_label, count in sorted(class_counts.items()):\n",
        "    class_name = class_names[class_label]\n",
        "    print(f\"Class {class_label} ({class_name}): {count} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 681
        },
        "id": "87hzgSUf2dEb",
        "outputId": "95497c8c-c817-44d9-9a23-c67a9d9256e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image shape: torch.Size([1, 32, 32])\n",
            "Image shape: torch.Size([1, 32, 32])\n",
            "Image shape: torch.Size([1, 32, 32])\n",
            "Image shape: torch.Size([1, 32, 32])\n",
            "Image shape: torch.Size([1, 32, 32])\n",
            "Image shape: torch.Size([1, 32, 32])\n",
            "Image shape: torch.Size([1, 32, 32])\n",
            "Image shape: torch.Size([1, 32, 32])\n",
            "Image shape: torch.Size([1, 32, 32])\n",
            "Image shape: torch.Size([1, 32, 32])\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1500x500 with 10 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Create a dictionary to store class indices and their corresponding images\n",
        "class_images = defaultdict(list)\n",
        "\n",
        "# Iterate through the display loader and collect one image per class\n",
        "display_iter = iter(displayloader)\n",
        "for images, labels in display_iter:\n",
        "    for image, label in zip(images, labels):\n",
        "        class_images[label.item()].append(image)\n",
        "        if len(class_images) == 10:  # Break once we have an image for each class\n",
        "            break\n",
        "\n",
        "# Create a figure to display the sample images\n",
        "fig = plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Iterate through the collected images and display them\n",
        "for class_label, images in class_images.items():\n",
        "    ax = fig.add_subplot(2, 5, class_label + 1, xticks=[], yticks=[])\n",
        "    ax.imshow(np.squeeze(images[0]), cmap='gray')\n",
        "    ax.set_title(class_names[class_label])  # Use class names from the dictionary\n",
        "\n",
        "    print(f\"Image shape: {images[0].shape}\")\n",
        "\n",
        "    fig.tight_layout()\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_laAV6P4ayP"
      },
      "source": [
        "### Data Loading\n",
        "\n",
        "FashionMNIST dataset loading and augmentation according to the SOTA implementation requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jcdpNAdt3fk8"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'pl' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\espda\\OneDrive\\Desktop\\Università\\Magistrale - Artificial Intelligence & Robotics (2022-2023)\\Deep Learning\\Progetto\\main_(5).ipynb Cell 14\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/espda/OneDrive/Desktop/Universit%C3%A0/Magistrale%20-%20Artificial%20Intelligence%20%26%20Robotics%20%282022-2023%29/Deep%20Learning/Progetto/main_%285%29.ipynb#X56sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mDataModule\u001b[39;00m(pl\u001b[39m.\u001b[39mLightningDataModule):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/espda/OneDrive/Desktop/Universit%C3%A0/Magistrale%20-%20Artificial%20Intelligence%20%26%20Robotics%20%282022-2023%29/Deep%20Learning/Progetto/main_%285%29.ipynb#X56sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch_size, data_dir: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/espda/OneDrive/Desktop/Universit%C3%A0/Magistrale%20-%20Artificial%20Intelligence%20%26%20Robotics%20%282022-2023%29/Deep%20Learning/Progetto/main_%285%29.ipynb#X56sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
            "\u001b[1;31mNameError\u001b[0m: name 'pl' is not defined"
          ]
        }
      ],
      "source": [
        "class DataModule(pl.LightningDataModule):\n",
        "\n",
        "    def __init__(self, batch_size, data_dir: str = \"./\"):\n",
        "        super().__init__()\n",
        "\n",
        "        class Rotation3:\n",
        "            def __call__(self, img):\n",
        "                # Randomly rotate the images by 90, 180, or 270 degrees\n",
        "                angle = random.choice([0, 90, 180, 270])\n",
        "                img = T.functional.rotate(img, angle)\n",
        "                return img\n",
        "\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = {\n",
        "                          'train': T.Compose([\n",
        "                              T.Resize((32)),\n",
        "                              T.RandomVerticalFlip(),\n",
        "                              T.RandomHorizontalFlip(),\n",
        "                              T.RandomCrop(30),\n",
        "                              T.Resize(32),\n",
        "                              T.Grayscale(1),  # Use 1 channel (grayscale)\n",
        "                              T.Resize(32),\n",
        "                              T.RandomApply([Rotation3()], p=1),\n",
        "                              T.Resize(32),\n",
        "                              T.ToTensor(),\n",
        "                              T.Normalize([0.5], [0.5])\n",
        "                          ]),\n",
        "                          'test': T.Compose([\n",
        "                              T.Resize(32),\n",
        "                              T.Grayscale(1),  # Use 1 channel (grayscale)\n",
        "                              T.Resize(32),\n",
        "                              T.ToTensor(),\n",
        "                              T.Normalize([0.5], [0.5])\n",
        "                          ]),\n",
        "                          'display': T.Compose([\n",
        "                              T.Resize(32),\n",
        "                              T.Grayscale(1),\n",
        "                              T.ToTensor(),\n",
        "                              T.Normalize([0.5], [0.5])\n",
        "                          ])\n",
        "                      }\n",
        "\n",
        "        #self.dims = (3, 32, 32)\n",
        "        self.num_classes = 10\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def prepare_data(self):\n",
        "        # download\n",
        "        FashionMNIST(root=self.data_dir, train=True, download=True, transform=self.transform['train'])\n",
        "        FashionMNIST(root=self.data_dir, train=False, download=True,transform=self.transform['test'])\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "\n",
        "        # Assign train/val datasets for use in dataloaders\n",
        "        if stage == 'fit' or stage is None:\n",
        "            # Define the transformations, train and validation datasets, and samplers here\n",
        "            full_dataset = FashionMNIST(root='./data', train=True, download=True, transform=self.transforms['train'])\n",
        "\n",
        "            indices = list(range(len(full_dataset)))\n",
        "            split = int(np.floor(0.2 * len(full_dataset)))\n",
        "\n",
        "            self.train_sampler = SubsetRandomSampler(indices[:split])\n",
        "            self.valid_sampler = SubsetRandomSampler(indices[split:])\n",
        "\n",
        "            self.train_dataset = torch.utils.data.Subset(full_dataset, indices[:split])\n",
        "            self.valid_dataset = torch.utils.data.Subset(full_dataset, indices[split:])\n",
        "\n",
        "        # Assign test dataset for use in dataloader(s)\n",
        "        if stage == \"test\" or stage is None:\n",
        "            self.test_dataset = FashionMNIST(self.data_dir, train=False, download=True,transform=self.transform['test'])\n",
        "        if stage == \"predict\" or stage is None:\n",
        "            self.test_dataset = FashionMNIST(self.data_dir, train=False, download=True,transform=self.transform['test'])\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(self.train_dataset, batch_size=self.batch_size)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return  torch.utils.data.DataLoader(self.valid_dataset, batch_size=self.batch_size)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return  torch.utils.data.DataLoader(self.test_dataset, batch_size=self.batch_size)\n",
        "\n",
        "    def predict_dataloader(self):\n",
        "        return  torch.utils.data.DataLoader(self.test_dataset, batch_size=self.batch_size)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyyQQm_vCuxX"
      },
      "outputs": [],
      "source": [
        "#import sklearn\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "class BalancedDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, batch_size, data_dir=\"./\", num_clusters=10):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_clusters = num_clusters\n",
        "\n",
        "        class Rotation3:\n",
        "            def __call__(self, img):\n",
        "                # Randomly rotate the images by 90, 180, or 270 degrees\n",
        "                angle = random.choice([0, 90, 180, 270])\n",
        "                img = T.functional.rotate(img, angle)\n",
        "                return img\n",
        "\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = {\n",
        "                          'train': T.Compose([\n",
        "                              T.Resize((32)),\n",
        "                              T.RandomVerticalFlip(),\n",
        "                              T.RandomHorizontalFlip(),\n",
        "                              T.RandomCrop(30),\n",
        "                              T.Resize(32),\n",
        "                              T.Grayscale(1),  # Use 1 channel (grayscale)\n",
        "                              T.Resize(32),\n",
        "                              T.RandomApply([Rotation3()], p=1),\n",
        "                              T.Resize(32),\n",
        "                              T.ToTensor(),\n",
        "                              T.Normalize([0.5], [0.5])\n",
        "                          ]),\n",
        "                          'test': T.Compose([\n",
        "                              T.Resize(32),\n",
        "                              T.Grayscale(1),  # Use 1 channel (grayscale)\n",
        "                              T.Resize(32),\n",
        "                              T.ToTensor(),\n",
        "                              T.Normalize([0.5], [0.5])\n",
        "                          ]),\n",
        "                          'display': T.Compose([\n",
        "                              T.Resize(32),\n",
        "                              T.Grayscale(1),\n",
        "                              T.ToTensor(),\n",
        "                              T.Normalize([0.5], [0.5])\n",
        "                          ])\n",
        "                      }\n",
        "        self.num_classes = 2\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def prepare_data(self):\n",
        "        # Download and preprocess the data\n",
        "        full_dataset = FashionMNIST(root=self.data_dir, train=True, download=True, transform=self.transform['train'])\n",
        "        self.full_dataset = full_dataset\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        if stage == 'fit' or stage is None:\n",
        "            # K-means clustering\n",
        "            all_data = [self.full_dataset[i][0].numpy().flatten() for i in range(len(self.full_dataset))]\n",
        "            kmeans = KMeans(n_clusters=self.num_clusters, random_state=0).fit(all_data)\n",
        "            cluster_indices = defaultdict(list)\n",
        "            for idx, label in enumerate(kmeans.labels_):\n",
        "                cluster_indices[label].append(idx)\n",
        "\n",
        "            normal_class = 0  # Define a class as normal\n",
        "            self.train_dataset = []\n",
        "            self.valid_dataset = []\n",
        "            for cls in range(self.num_classes):\n",
        "                if cls == normal_class:\n",
        "                    # Use all data for the normal class\n",
        "                    indices = cluster_indices[cls]\n",
        "                else:\n",
        "                    # Sample the same number of data from each cluster for anomaly classes\n",
        "                    sampled_indices = shuffle(cluster_indices[cls], random_state=0)[:len(cluster_indices[normal_class])]\n",
        "                    indices = sampled_indices\n",
        "\n",
        "                split = int(np.floor(0.2 * len(indices)))\n",
        "                self.train_sampler = SubsetRandomSampler(indices[:split])\n",
        "                self.valid_sampler = SubsetRandomSampler(indices[split:])\n",
        "\n",
        "                self.train_dataset.extend(torch.utils.data.Subset(self.full_dataset, indices[:split]))\n",
        "                self.valid_dataset.extend(torch.utils.data.Subset(self.full_dataset, indices[split:]))\n",
        "                normal_class += 1\n",
        "\n",
        "        if stage == \"test\" or stage is None:\n",
        "            self.test_dataset = FashionMNIST(self.data_dir, train=False, download=True, transform=self.transform['test'])\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(self.train_dataset, batch_size=self.batch_size)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return  torch.utils.data.DataLoader(self.valid_dataset, batch_size=self.batch_size)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return  torch.utils.data.DataLoader(self.test_dataset, batch_size=self.batch_size)\n",
        "\n",
        "    def predict_dataloader(self):\n",
        "        return  torch.utils.data.DataLoader(self.test_dataset, batch_size=self.batch_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kG1iyc556-nd"
      },
      "source": [
        "### Simple Model FC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "465FlfMP63XV"
      },
      "outputs": [],
      "source": [
        "class PrintAccuracyAndLoss(pl.Callback):\n",
        "    def on_epoch_end(self, trainer, pl_module):\n",
        "        train_acc = pl_module.train_acc.compute()\n",
        "        val_acc = pl_module.val_acc.compute()\n",
        "        train_loss = trainer.callback__metrics['train_loss']\n",
        "        val_loss = trainer.callback__metrics['val_loss']\n",
        "\n",
        "        print(f\"Epoch {trainer.current_epoch}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}\")\n",
        "\n",
        "\n",
        "class SimpleCNNModel(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.l1 = torch.nn.Linear(32 * 16 * 16, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Convolution\n",
        "        x = self.conv1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = torch.max_pool2d(x, 2)\n",
        "\n",
        "        # FC\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.l1(x)\n",
        "        x = torch.relu(x)\n",
        "        return x\n",
        "\n",
        "    def training_step(self, batch, batch_nb):\n",
        "        x, y = batch\n",
        "        pred = self(x)\n",
        "        #loss = F.cross_entropy(pred, y)\n",
        "        loss = F.mse_loss(pred, y)\n",
        "        acc = (pred.argmax(dim=-1) == y).float().mean()\n",
        "\n",
        "        self.log(\"train_acc\", acc, on_step=True, on_epoch=True)\n",
        "        self.log(\"train Loss:\", loss)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_nb):\n",
        "        x, y = batch\n",
        "        pred = self(x)\n",
        "        #loss = F.cross_entropy(pred, y)\n",
        "        loss = F.mse_loss(pred, y)\n",
        "        acc = (pred.argmax(dim=-1) == y).float().mean()\n",
        "\n",
        "        self.log(\"valid_acc\", acc, on_step=True, on_epoch=True)\n",
        "        self.log(\"valid Loss:\", loss)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_nb):\n",
        "        x, y = batch\n",
        "        pred = self(x)\n",
        "        #loss = F.cross_entropy(pred, y)\n",
        "        loss = F.mse_loss(pred, y)\n",
        "        acc = (pred.argmax(dim=-1) == y).float().mean()\n",
        "\n",
        "        self.log(\"test_acc\", acc, on_step=True, on_epoch=True)\n",
        "        self.log(\"test Loss:\", loss)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=0.02)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4gcvVPpDEVu"
      },
      "source": [
        "## **Training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOQBdegoDIV4"
      },
      "source": [
        "### Train 10 classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d127d6862c0e4437bbe8e1d1c89c053b",
            "a8f1d7ef74b949fda2016983cb9e0a4a",
            "8ac27229ec2142e2a72ab7a10fc86a65",
            "75b27c66405d4732b5d1f57f4e6051b2",
            "7b4d9902e0524e82b4159e56062a5565",
            "1306bcb68dfe45d6bd899b8473730d8b",
            "f1c98dad392643c8936617d87a75aee9",
            "8b1037bcb95949abb5bc5dfaba96331e",
            "519f2e73d9ab45a882ac871553ebf7fa",
            "a579aa90896140c39a818d7092067bcd",
            "d1e952e3caa44990afbf5580390cc182"
          ]
        },
        "id": "gh34gSqS7Ey2",
        "outputId": "fccabaaa-0b21-4101-ff6b-75f4bbfd55ba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 26421880/26421880 [00:01<00:00, 17440219.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./FashionMNIST/raw/train-images-idx3-ubyte.gz to ./FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 300957.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4422102/4422102 [00:00<00:00, 5598243.14it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 18298539.82it/s]\n",
            "WARNING:pytorch_lightning.loggers.tensorboard:Missing logger folder: /content/lightning_logs\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 26421880/26421880 [00:01<00:00, 16431275.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 302767.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4422102/4422102 [00:00<00:00, 5598690.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 8138815.30it/s]\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name  | Type   | Params\n",
            "---------------------------------\n",
            "0 | conv1 | Conv2d | 320   \n",
            "1 | l1    | Linear | 16.4 K\n",
            "---------------------------------\n",
            "16.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "16.7 K    Total params\n",
            "0.067     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d127d6862c0e4437bbe8e1d1c89c053b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Sanity Checking: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "IndexError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-76c72c48ccbe>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                      \u001b[0;31m#logger=logger,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                      callbacks=[TQDMProgressBar(refresh_rate=20), PrintAccuracyAndLoss(), EarlyStopping(monitor=\"valid_acc\", patience=3)],)  #`train_acc`, `train_acc_step`, `Train Loss:`, `Valid_acc`, `Valid_acc_epoch`, `Valid Loss:`, `train_acc_epoch`\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m#trainer.fit(model, trainloader, valloader)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#trainer.fit(model, trainloader)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_maybe_unwrap_optimized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lightning_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m         call._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    530\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_TunerExitException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    566\u001b[0m             \u001b[0mmodel_connected\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         )\n\u001b[0;32m--> 568\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    971\u001b[0m         \u001b[0;31m# RUN THE TRAINER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0misolate_rng\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_sanity_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_detect_anomaly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m             \u001b[0;31m# run eval step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1043\u001b[0;31m             \u001b[0mval_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m             \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_callback_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"on_sanity_check_end\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py\u001b[0m in \u001b[0;36m_decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mcontext_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcontext_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mloop_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_decorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0mprevious_dataloader_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0;31m# run step hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0;31m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\u001b[0m in \u001b[0;36m_evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0mhook_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"test_step\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"validation_step\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_strategy_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mstep_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_progress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincrement_processed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[Strategy]{trainer.strategy.__class__.__name__}.{hook_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;31m# restore current_fx when nested context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/strategies/strategy.py\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_step_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValidationStep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSTEP_OUTPUT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-1764643a3c2b>\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(self, batch, batch_nb)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3028\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3029\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: Target 6 is out of bounds."
          ]
        }
      ],
      "source": [
        "dm = DataModule(128)\n",
        "\n",
        "model = SimpleCNNModel()\n",
        "trainer = pl.Trainer(max_epochs=2,\n",
        "                     #logger=logger,\n",
        "                     callbacks=[TQDMProgressBar(refresh_rate=20),\n",
        "                                PrintAccuracyAndLoss(),\n",
        "                                EarlyStopping(monitor=\"valid_acc\", patience=3)],)  #`train_acc`, `train_acc_step`, `Train Loss:`, `Valid_acc`, `Valid_acc_epoch`, `Valid Loss:`, `train_acc_epoch`\n",
        "res = trainer.fit(model, dm)\n",
        "#trainer.fit(model, trainloader, valloader)\n",
        "#trainer.fit(model, trainloader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ub0iwRDYa7k3"
      },
      "outputs": [],
      "source": [
        "res = trainer.validate(model, datamodule=dm)\n",
        "#print(res)\n",
        "\n",
        "#model = MNISTModel()\n",
        "res = trainer.test(model, datamodule=dm)\n",
        "#print(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDmufRmODLhw"
      },
      "source": [
        "### Train 2 classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c5a7ff9ae2db49b681a8c4a8e89a2fc0",
            "529290062b26453eb72028d3cef9af95",
            "c8c85607e00d463ebf8f3bcb66443dfb",
            "fa12ff57f0724c9eb451b8075811be2c",
            "2b84db9a5d6c4031bc16deb65b357a95",
            "cf8a684590324c75b9903c312c87f2c9",
            "28bf6b7fca784dc19e69c1c93d82fa8f",
            "6dbde2d93c4e4c599ef3f5a91fe762b6",
            "014f81a6caac4aac970ba5315c042d3a",
            "4c370e5a291f46f290a04fb1b899dfba",
            "47d3a3c58f474cb6bdc3f1a47cf383bd"
          ]
        },
        "id": "YcYNMTblDNUo",
        "outputId": "c9cdb2d5-16f2-48eb-f80f-3c3d728b74ca"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "c:\\Users\\espda\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py:68: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
            "  \"Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning`\"\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "The `Callback.on_epoch_end` hook was removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21120\\57670159.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m                                 EarlyStopping(monitor=\"valid_acc\", patience=3)],)  #`train_acc`, `train_acc_step`, `Train Loss:`, `Valid_acc`, `Valid_acc_epoch`, `Valid Loss:`, `train_acc_epoch`\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbalanced_dm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbalanced_dm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\espda\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    607\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lightning_module\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m         call._call_and_handle_interrupt(\n\u001b[1;32m--> 609\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    610\u001b[0m         )\n\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\espda\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_TunerExitException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\espda\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    648\u001b[0m             \u001b[0mmodel_connected\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlightning_module\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    649\u001b[0m         )\n\u001b[1;32m--> 650\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    651\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    652\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\espda\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m   1036\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callback_connector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_attach_model_logging_functions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1037\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1038\u001b[1;33m         \u001b[0mverify_loop_configurations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1039\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1040\u001b[0m         \u001b[1;31m# hook\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\espda\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py\u001b[0m in \u001b[0;36mverify_loop_configurations\u001b[1;34m(trainer)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0m__verify_batch_transfer_support\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;31m# TODO: Delete this check in v2.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m     \u001b[0m_check_deprecated_callback_hooks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m     \u001b[1;31m# TODO: Delete this check in v2.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[0m_check_on_epoch_start_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\espda\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py\u001b[0m in \u001b[0;36m_check_deprecated_callback_hooks\u001b[1;34m(trainer)\u001b[0m\n\u001b[0;32m    254\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m                 raise RuntimeError(\n\u001b[1;32m--> 256\u001b[1;33m                     \u001b[1;34mf\"The `Callback.{hook}` hook was removed in v1.8. Please use `Callback.{alternative_hook}` instead.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    257\u001b[0m                 )\n\u001b[0;32m    258\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"on_pretrain_routine_start\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"on_pretrain_routine_end\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: The `Callback.on_epoch_end` hook was removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead."
          ]
        }
      ],
      "source": [
        "balanced_dm = BalancedDataModule(batch_size=128)\n",
        "#balanced_dm.prepare_data()\n",
        "#balanced_dm.setup()\n",
        "\n",
        "model = SimpleCNNModel()\n",
        "trainer = pl.Trainer(max_epochs=2,\n",
        "                     #logger=logger,\n",
        "                     callbacks=[TQDMProgressBar(refresh_rate=20),\n",
        "                                PrintAccuracyAndLoss(),\n",
        "                                EarlyStopping(monitor=\"valid_acc\", patience=3)],)  #`train_acc`, `train_acc_step`, `Train Loss:`, `Valid_acc`, `Valid_acc_epoch`, `Valid Loss:`, `train_acc_epoch`\n",
        "\n",
        "res = trainer.fit(model, balanced_dm)\n",
        "res = trainer.test(model, datamodule=balanced_dm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crjpDsR-lLAk"
      },
      "source": [
        "## DataModule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBOKPgXLI8yh",
        "outputId": "3e0b6136-429f-4874-8d79-8bd7681ad549"
      },
      "outputs": [],
      "source": [
        "#!pip install --quiet \"pytorch-lightning\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pytorch_lightning as pl\n",
        "import torchvision.datasets\n",
        "import torchvision.transforms as T\n",
        "from torchvision.datasets import FashionMNIST\n",
        "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
        "from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Set float32 matrix multiplication precision for better performance\n",
        "#torch.set_float32_matmul_precision('medium')  # or 'high' if desired\n",
        "\n",
        "\n",
        "class Rotation3:\n",
        "    def __call__(self, img):\n",
        "        # Randomly rotate the images by 90, 180, or 270 degrees\n",
        "        angle = random.choice([0, 90, 180, 270])\n",
        "        img = T.functional.rotate(img, angle)\n",
        "        return img\n",
        "\n",
        "class AnomalyDetectionDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, batch_size, subsampling, subsamples, data_dir=\"./\"):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        self.transform = T.Compose([\n",
        "            T.Resize((32, 32)),\n",
        "            T.Grayscale(1),\n",
        "            T.ToTensor(),\n",
        "            T.Normalize([0.5], [0.5])\n",
        "        ])\n",
        "        self.train_transform = T.Compose([\n",
        "                                          T.Resize(32),\n",
        "                                          T.RandomVerticalFlip(),\n",
        "                                          T.RandomHorizontalFlip(),\n",
        "                                          T.RandomCrop(30),\n",
        "                                          T.Resize(32),\n",
        "                                          T.Grayscale(1),  # Use 1 channel (grayscale)\n",
        "                                          T.RandomApply([Rotation3()], p=1),\n",
        "                                          T.ToTensor(),\n",
        "                                          T.Normalize([0.5], [0.5])\n",
        "                                         ]),\n",
        "        self.test_transform = T.Compose([\n",
        "                                          T.Resize(32),\n",
        "                                          T.Grayscale(1),  # Use 1 channel (grayscale)\n",
        "                                          T.ToTensor(),\n",
        "                                          T.Normalize([0.5], [0.5])\n",
        "                                        ])\n",
        "\n",
        "        self.data_dir = data_dir\n",
        "        self.dataset_classes = 10\n",
        "        self.num_classes = 2  # Normal and Anomaly\n",
        "        self.batch_size = batch_size\n",
        "        self.num_clusters = self.dataset_classes - 1\n",
        "        self.subsamples = subsamples\n",
        "        self.subsampling = subsampling\n",
        "\n",
        "\n",
        "    def kmeans_sampling(self, dataset, class_idx, num_samples):\n",
        "        indices = [i for i in range(len(dataset)) if dataset.targets[i] != class_idx]\n",
        "        class_data = torch.stack([dataset[i][0] for i in indices])\n",
        "        kmeans = KMeans(n_clusters=self.num_clusters, random_state=0, n_init=1) #n_init = 9\n",
        "        kmeans.fit(class_data.view(class_data.size(0), -1).numpy())\n",
        "        cluster_assignments = kmeans.predict(class_data.view(class_data.size(0), -1).numpy())\n",
        "        sampled_indices = []\n",
        "        for cluster_idx in range(self.num_clusters):\n",
        "            cluster_indices = np.where(cluster_assignments == cluster_idx)[0]\n",
        "            num_samples_from_cluster = min(num_samples, len(cluster_indices))\n",
        "            sampled_indices.extend(np.random.choice(cluster_indices, num_samples_from_cluster, replace=False))\n",
        "        return [indices[idx] for idx in sampled_indices]\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        if stage == 'fit' or stage is None:\n",
        "            all_data = []\n",
        "            all_labels = []\n",
        "\n",
        "            for normal_class in range(self.dataset_classes):\n",
        "                dataset = FashionMNIST(root=self.data_dir, train=True, download=True, transform=self.test_transform)\n",
        "\n",
        "                normal_indices = [i for i in range(len(dataset)) if dataset.targets[i] == normal_class]\n",
        "                if self.subsampling == True:\n",
        "                    normal_indices = normal_indices[:self.subsamples]\n",
        "                #anomaly_indices = [i for i in range(len(dataset)) if dataset.targets[i] != normal_class]\n",
        "                anomaly_indices = self.kmeans_sampling(dataset, normal_class, num_samples=len(normal_indices))\n",
        "\n",
        "                normal_labels = torch.zeros(len(normal_indices), dtype=torch.float32)\n",
        "                anomaly_labels = torch.ones(len(anomaly_indices), dtype=torch.float32)\n",
        "\n",
        "                all_data.extend([dataset[i][0] for i in normal_indices])\n",
        "                all_data.extend([dataset[i][0] for i in anomaly_indices])\n",
        "                all_labels.extend(normal_labels)\n",
        "                all_labels.extend(anomaly_labels)\n",
        "\n",
        "            all_data = torch.stack(all_data)\n",
        "            all_labels = torch.tensor(all_labels)\n",
        "\n",
        "            self.train_dataset = torch.utils.data.TensorDataset(all_data, all_labels)\n",
        "\n",
        "\n",
        "\n",
        "        if stage == 'test' or stage is None:\n",
        "            all_data = []\n",
        "            all_labels = []\n",
        "\n",
        "            for class_idx in range(self.dataset_classes):\n",
        "                dataset = FashionMNIST(root=self.data_dir, train=False, download=True, transform=self.transform)\n",
        "                normal_indices = [i for i in range(len(dataset)) if dataset.targets[i] == class_idx]\n",
        "                anomaly_indices = [i for i in range(len(dataset)) if dataset.targets[i] != class_idx]\n",
        "\n",
        "                normal_indices = normal_indices[:100]\n",
        "                anomaly_indices = anomaly_indices[:100]\n",
        "\n",
        "                normal_labels = torch.zeros(len(normal_indices), dtype=torch.float32)  # Set labels to Float data type\n",
        "                anomaly_labels = torch.ones(len(anomaly_indices), dtype=torch.float32)  # Set labels to Float data type\n",
        "\n",
        "                all_data.extend([dataset[i][0] for i in normal_indices])\n",
        "                all_data.extend([dataset[i][0] for i in anomaly_indices])\n",
        "                all_labels.extend(normal_labels)\n",
        "                all_labels.extend(anomaly_labels)\n",
        "\n",
        "            all_data = torch.stack(all_data)\n",
        "            all_labels = torch.tensor(all_labels)\n",
        "\n",
        "            self.test_dataset = torch.utils.data.TensorDataset(all_data, all_labels)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(self.train_dataset, batch_size=self.batch_size)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(self.test_dataset, batch_size=self.batch_size)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3c-AIe7kn20"
      },
      "source": [
        "## Simple CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "q8PKu1VjkmtR"
      },
      "outputs": [],
      "source": [
        "class SimpleCNNModel(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.fc = nn.Linear(32 * 16 * 16, 1)  # Output a single value\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = torch.max_pool2d(x, 2)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        pred = self(x)\n",
        "\n",
        "        #print(\"pred\", pred.squeeze(), \"shape:\",pred.squeeze().shape, \"|||    y\", y.squeeze(), \"shape:\", y.shape)\n",
        "        loss = F.mse_loss(pred.squeeze(), y.squeeze())  # Calculate mean squared error loss\n",
        "        acc = (pred.round() == y).float().mean()\n",
        "\n",
        "        self.log(\"train_loss\", loss)\n",
        "        self.log(\"train_acc\", acc)\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        pred = self(x)\n",
        "\n",
        "        loss = F.mse_loss(pred.squeeze(), y.squeeze())\n",
        "        acc = (pred.round() == y).float().mean()\n",
        "\n",
        "        self.log(\"test_loss\", loss)\n",
        "        self.log(\"test_acc\", acc)\n",
        "\n",
        "        # Calculate ROC-AUC\n",
        "        y_prob = torch.sigmoid(pred.squeeze())  # Apply sigmoid to get probability scores\n",
        "        roc_auc = roc_auc_score(y.cpu().numpy(), y_prob.cpu().numpy())\n",
        "\n",
        "        self.log(\"roc_auc\", roc_auc)\n",
        "\n",
        "        return {\"test_loss\": loss, \"test_acc\": acc, \"test_roc_auc\": roc_auc}\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=0.002)\n",
        "\n",
        "\n",
        "class SOTA(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.fc = nn.Linear(32 * 16 * 16, 1)  # Output a single value\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = torch.max_pool2d(x, 2)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mZHVln5hak-"
      },
      "source": [
        "### Train and Test CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsEygthjhc0v",
        "outputId": "878f357d-bf31-4f72-b2d9-42696351fbd1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name  | Type   | Params\n",
            "---------------------------------\n",
            "0 | conv1 | Conv2d | 320   \n",
            "1 | fc    | Linear | 8.2 K \n",
            "---------------------------------\n",
            "8.5 K     Trainable params\n",
            "0         Non-trainable params\n",
            "8.5 K     Total params\n",
            "0.034     Total estimated model params size (MB)\n",
            "c:\\Users\\espda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:438: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  rank_zero_warn(\n",
            "c:\\Users\\espda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:281: PossibleUserWarning: The number of training batches (40) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "  rank_zero_warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: 100%|██████████| 40/40 [00:01<00:00, 22.99it/s, v_num=19]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: 100%|██████████| 40/40 [00:01<00:00, 22.91it/s, v_num=19]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "c:\\Users\\espda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:438: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  rank_zero_warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing DataLoader 0: 100%|██████████| 8/8 [00:00<00:00, 153.72it/s]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">          roc_auc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.4822308863039413     </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.5            </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.43182921409606934    </span>│\n",
              "└───────────────────────────┴───────────────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│\u001b[36m \u001b[0m\u001b[36m         roc_auc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.4822308863039413    \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.5           \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.43182921409606934   \u001b[0m\u001b[35m \u001b[0m│\n",
              "└───────────────────────────┴───────────────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'test_loss': 0.43182921409606934, 'test_acc': 0.5, 'roc_auc': 0.4822308863039413}]\n"
          ]
        }
      ],
      "source": [
        "#Hyperparameters\n",
        "BATCH_SIZE = 256\n",
        "epochs = 1\n",
        "subsamples = 100\n",
        "accelerator_enabled = True\n",
        "\n",
        "data_module = AnomalyDetectionDataModule(batch_size=256, subsamples=subsamples)\n",
        "model = SimpleCNNModel()\n",
        "#logger = TensorBoardLogger(\"tb_logs\", name=\"net\",log_graph=True )\n",
        "\n",
        "\n",
        "if torch.cuda.is_available() and accelerator_enabled: \n",
        "  print(\"GPU\")\n",
        "  trainer = pl.Trainer(accelerator=\"cuda\", max_epochs=epochs, callbacks=[TQDMProgressBar(), EarlyStopping(monitor=\"train_loss\", patience=3)])\n",
        "else : \n",
        "  print(\"CPU\")\n",
        "  trainer = pl.Trainer(accelerator=\"cpu\", max_epochs=epochs, callbacks=[TQDMProgressBar(), EarlyStopping(monitor=\"train_loss\", patience=3)])\n",
        "\n",
        "\n",
        "trainer.fit(model, data_module)\n",
        "\n",
        "# Testing\n",
        "test_results = trainer.test(model, data_module)\n",
        "print(test_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rDsWkAg62GjT"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6006 (pid 17928), started 0:23:00 ago. (Use '!kill 17928' to kill it.)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-144ffc5dd0ab3f0c\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-144ffc5dd0ab3f0c\");\n",
              "          const url = new URL(\"http://localhost\");\n",
              "          const port = 6006;\n",
              "          if (port) {\n",
              "            url.port = port;\n",
              "          }\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir ./lightning_logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOJ3z_FPkqye"
      },
      "source": [
        "## SOTA GAN-based\n",
        "\n",
        "- **U-NET Structure**: Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedical image segmentation. In: International Conference on Medical image computing\n",
        "and computer-assisted intervention. pp. 234–241. Springer (2015)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWqfyQWmZKwe",
        "outputId": "c3edf80e-51e8-4be0-9386-36bd7eddf19d"
      },
      "outputs": [],
      "source": [
        "# Generator\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(Generator, self).__init__()\n",
        "        '''\n",
        "        # Encoder\n",
        "        self.enc_conv1 = self.conv_block(in_channels, 22)\n",
        "        self.enc_conv2 = self.conv_block(22, 32)\n",
        "        self.enc_conv3 = self.conv_block(32, 42)\n",
        "        \n",
        "        # Bottleneck\n",
        "        self.bottleneck = self.conv_block(42, 50)\n",
        "        \n",
        "        # Decoder\n",
        "        self.dec_conv1 = self.conv_block(50, 42, transpose=True)\n",
        "        self.dec_conv2 = self.conv_block(42, 32, transpose=True)\n",
        "        self.dec_conv3 = self.conv_block(32, 22, transpose=True)\n",
        "        \n",
        "        self.final_conv = self.conv_block(22, 1, transpose=True)\n",
        "'''\n",
        "        # Encoder\n",
        "        self.enc_conv1 = self.conv_block(in_channels, 64)\n",
        "        #self.enc_conv2 = self.conv_block(64, 128)\n",
        "        #self.enc_conv3 = self.conv_block(128, 256)\n",
        "        \n",
        "        # Bottleneck\n",
        "        #self.bottleneck = self.conv_block(256, 512)\n",
        "        \n",
        "        # Decoder\n",
        "        #self.dec_conv1 = self.conv_block(512, 256, transpose=True)\n",
        "        #self.dec_conv2 = self.conv_block(256, 128, transpose=True)\n",
        "        #self.dec_conv3 = self.conv_block(128, 64, transpose=True)\n",
        "        \n",
        "        self.final_conv = self.conv_block(64, 1, transpose=True)\n",
        "\n",
        "\n",
        "        print(\"GENERATOR:\")\n",
        "        self.print_parameter_count(\"Encoder Conv1\", self.enc_conv1)\n",
        "        #self.print_parameter_count(\"Encoder Conv2\", self.enc_conv2)\n",
        "        #self.print_parameter_count(\"Encoder Conv3\", self.enc_conv3)\n",
        "        #self.print_parameter_count(\"Bottleneck\", self.bottleneck)\n",
        "        #self.print_parameter_count(\"Decoder Conv1\", self.dec_conv1)\n",
        "        #self.print_parameter_count(\"Decoder Conv2\", self.dec_conv2)\n",
        "        #self.print_parameter_count(\"Decoder Conv3\", self.dec_conv3)\n",
        "        self.print_parameter_count(\"Final Conv\", self.final_conv)\n",
        "\n",
        "        \n",
        "    def conv_block(self, in_channels, out_channels, transpose=False):\n",
        "        if transpose:\n",
        "            return nn.Sequential(\n",
        "                nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1),\n",
        "                nn.ReLU(inplace=True),\n",
        "            )\n",
        "        else:\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1),\n",
        "                nn.ReLU(inplace=True),\n",
        "                #nn.MaxPool2d(2, stride=2)\n",
        "                #nn.Conv2d(out_channels, out_channels, kernel_size=4, stride=2),\n",
        "                #nn.ReLU(inplace=True)\n",
        "            )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        enc1 = self.enc_conv1(x)\n",
        "        #enc2 = self.enc_conv2(F.max_pool2d(enc1, kernel_size=2, stride=2))\n",
        "        #enc3 = self.enc_conv3(F.max_pool2d(enc2, kernel_size=2, stride=2))\n",
        "        \n",
        "        # Bottleneck\n",
        "        #bottleneck = self.bottleneck(F.max_pool2d(enc3,  kernel_size=2, stride=2))\n",
        "        \n",
        "        # Decoder\n",
        "        #dec1 = F.interpolate(self.dec_conv1(bottleneck), scale_factor=2, mode='bilinear', align_corners=False)\n",
        "        #dec2 = F.interpolate(self.dec_conv2(torch.cat([dec1, enc1], dim=1)), scale_factor=2, mode='bilinear', align_corners=False)\n",
        "        \n",
        "        #output = self.final_conv(dec2)\n",
        "        output = self.final_conv(enc1)\n",
        "        return output\n",
        "    \n",
        "    \n",
        "    def print_parameter_count(self, name, module):\n",
        "        num_params = sum(p.numel() for p in module.parameters())\n",
        "        formatted_params = self.format_parameter_count(num_params)\n",
        "        #print(f\"{name} - Number of Parameters: {num_params}\")\n",
        "        print(f\"{name} - Number of Parameters: {formatted_params}\")\n",
        "        \n",
        "    def format_parameter_count(self, num_params):\n",
        "        if num_params < 1e3:\n",
        "            return f\"{num_params:.2f}\"\n",
        "        elif num_params < 1e6:\n",
        "            return f\"{num_params / 1e3:.2f}K\"\n",
        "        elif num_params < 1e9:\n",
        "            return f\"{num_params / 1e6:.2f}M\"\n",
        "        else:\n",
        "            return f\"{num_params / 1e9:.2f}G\"\n",
        "        \n",
        "\n",
        "# Discriminator\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.conv1_s2 = self.conv_block(1, 64, kernel_size=3, stride=2)\n",
        "        #self.conv2_s2 = self.conv_block(22, 128, kernel_size=3, stride=2)\n",
        "        #self.conv3_s2 = self.conv_block(128, 64, kernel_size=3, stride=2)\n",
        "        #self.conv1_s1 = self.conv_block(64, 32, kernel_size=3, stride=1)\n",
        "        self.conv2_s1 = self.conv_block(64, 1, kernel_size=3, stride=1)\n",
        "\n",
        "        print(\"DISCRIMINATOR:\")\n",
        "        self.print_parameter_count(\"Conv1_s2\", self.conv1_s2)\n",
        "        #self.print_parameter_count(\"Conv2_s2\", self.conv2_s2)\n",
        "        #self.print_parameter_count(\"Conv3_s2\", self.conv3_s2)\n",
        "        #self.print_parameter_count(\"Conv1_s1\", self.conv1_s1)\n",
        "        self.print_parameter_count(\"Conv2_s1\", self.conv2_s1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1_s2(x)\n",
        "        #x = self.conv2_s2(x)\n",
        "        #x = self.conv1_s1(x)\n",
        "        x = self.conv2_s2(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    def conv_block(self, in_channels, out_channels, kernel_size, stride):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def print_parameter_count(self, name, module):\n",
        "        num_params = sum(p.numel() for p in module.parameters())\n",
        "        formatted_params = self.format_parameter_count(num_params)\n",
        "        print(f\"{name} - Number of Parameters: {num_params}\")\n",
        "        #print(f\"{name} - Number of Parameters: {formatted_params}\")\n",
        "        \n",
        "    def format_parameter_count(self, num_params):\n",
        "        if num_params < 1e3:\n",
        "            return f\"{num_params:.2f}\"\n",
        "        elif num_params < 1e6:\n",
        "            return f\"{num_params / 1e3:.2f}K\"\n",
        "        elif num_params < 1e9:\n",
        "            return f\"{num_params / 1e6:.2f}M\"\n",
        "        else:\n",
        "            return f\"{num_params / 1e9:.2f}G\"\n",
        "\n",
        "\n",
        "# GAN Model\n",
        "class GAN(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super(GAN, self).__init__()\n",
        "        self.generator = Generator(1, 1)\n",
        "        self.discriminator_normal = Discriminator()\n",
        "        self.discriminator_anomaly = Discriminator()\n",
        "\n",
        "        self.automatic_optimization = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.generator(x)\n",
        "\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        real_images, _ = batch\n",
        "        valid = torch.ones(real_images.size(0), 1)\n",
        "        fake = torch.zeros(real_images.size(0), 1)\n",
        "\n",
        "        z = torch.randn(real_images.size(0), 1, 1, 1)\n",
        "        gen_images = self.generator(z)\n",
        "\n",
        "        # Manually access optimizers\n",
        "        opt_gen, opt_disc_normal, opt_disc_anomaly = self.optimizers()\n",
        "\n",
        "        # Train Generator\n",
        "        gen_loss = F.binary_cross_entropy(self.discriminator_normal(gen_images), valid)\n",
        "        self.log(\"gen_loss\", gen_loss, on_step=True, on_epoch=True)\n",
        "        opt_gen.zero_grad()\n",
        "        gen_loss.backward()\n",
        "        opt_gen.step()\n",
        "\n",
        "        # Train Discriminator (Normal)\n",
        "        real_loss = F.binary_cross_entropy(self.discriminator_normal(real_images), valid)\n",
        "        fake_loss = F.binary_cross_entropy(self.discriminator_normal(gen_images.detach()), fake)\n",
        "        \n",
        "        #LSGAN Discriminator Loss\n",
        "        disc_loss_normal = (real_loss + fake_loss) / 2\n",
        "        self.log(\"disc_loss_normal\", disc_loss_normal, on_step=True, on_epoch=True)\n",
        "        opt_disc_normal.zero_grad()\n",
        "        disc_loss_normal.backward()\n",
        "        opt_disc_normal.step()\n",
        "\n",
        "        # Train Discriminator (Anomaly)\n",
        "        real_loss = F.binary_cross_entropy(self.discriminator_anomaly(real_images), valid)\n",
        "        fake_loss = F.binary_cross_entropy(self.discriminator_anomaly(gen_images.detach()), fake)\n",
        "        disc_loss_anomaly = (real_loss + fake_loss) / 2\n",
        "        self.log(\"disc_loss_anomaly\", disc_loss_anomaly, on_step=True, on_epoch=True)\n",
        "        opt_disc_anomaly.zero_grad()\n",
        "        disc_loss_anomaly.backward()\n",
        "        opt_disc_anomaly.step()\n",
        "\n",
        "        return {\"gen_loss\": gen_loss, \"disc_loss_normal\": disc_loss_normal, \"disc_loss_anomaly\": disc_loss_anomaly}\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        real_images, _ = batch\n",
        "        z = torch.randn(real_images.size(0), 1, 1, 1)\n",
        "        gen_images = self.generator(z)\n",
        "\n",
        "        # Calculate any metrics you need for testing\n",
        "        # For example, you can use FID or any other appropriate metrics\n",
        "\n",
        "        return {\"gen_images\": gen_images}\n",
        "\n",
        "    def test_epoch_end(self, outputs):\n",
        "        # Calculate aggregate metrics from test_step results\n",
        "        # For example, calculate the average FID over the test dataset\n",
        "\n",
        "        #avg_fid = calculate_average_fid(outputs)  # Replace with your actual metric calculation\n",
        "\n",
        "        #self.log(\"avg_fid\", avg_fid, on_epoch=True)\n",
        "        self.log(\"avg_fid\")\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        opt_gen = torch.optim.Adam(self.generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "        opt_disc_normal = torch.optim.Adam(self.discriminator_normal.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "        opt_disc_anomaly = torch.optim.Adam(self.discriminator_anomaly.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "        return [opt_gen, opt_disc_normal, opt_disc_anomaly]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mP1zaIFhhoS"
      },
      "source": [
        "### Train and Test SOTA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GENERATOR:\n",
            "Encoder Conv1 - Number of Parameters: 1.09K\n",
            "Final Conv - Number of Parameters: 1.02K\n",
            "DISCRIMINATOR:\n",
            "Conv1_s2 - Number of Parameters: 640\n",
            "Conv2_s1 - Number of Parameters: 577\n",
            "DISCRIMINATOR:\n",
            "Conv1_s2 - Number of Parameters: 640\n",
            "Conv2_s1 - Number of Parameters: 577\n",
            "GPU\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name                  | Type          | Params\n",
            "--------------------------------------------------------\n",
            "0 | generator             | Generator     | 2.1 K \n",
            "1 | discriminator_normal  | Discriminator | 1.2 K \n",
            "2 | discriminator_anomaly | Discriminator | 1.2 K \n",
            "--------------------------------------------------------\n",
            "4.5 K     Trainable params\n",
            "0         Non-trainable params\n",
            "4.5 K     Total params\n",
            "0.018     Total estimated model params size (MB)\n",
            "c:\\Users\\espda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:438: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  rank_zero_warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0:   0%|          | 0/100 [00:00<?, ?it/s] "
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Calculated padded input size per channel: (3 x 3). Kernel size: (4 x 4). Kernel size can't be greater than actual input size",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\espda\\OneDrive\\Desktop\\Università\\Magistrale - Artificial Intelligence & Robotics (2022-2023)\\Deep Learning\\Progetto\\main_(5).ipynb Cell 34\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/espda/OneDrive/Desktop/Universit%C3%A0/Magistrale%20-%20Artificial%20Intelligence%20%26%20Robotics%20%282022-2023%29/Deep%20Learning/Progetto/main_%285%29.ipynb#X45sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCPU\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/espda/OneDrive/Desktop/Universit%C3%A0/Magistrale%20-%20Artificial%20Intelligence%20%26%20Robotics%20%282022-2023%29/Deep%20Learning/Progetto/main_%285%29.ipynb#X45sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m   trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(accelerator\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/espda/OneDrive/Desktop/Universit%C3%A0/Magistrale%20-%20Artificial%20Intelligence%20%26%20Robotics%20%282022-2023%29/Deep%20Learning/Progetto/main_%285%29.ipynb#X45sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m                        max_epochs\u001b[39m=\u001b[39mepochs, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/espda/OneDrive/Desktop/Universit%C3%A0/Magistrale%20-%20Artificial%20Intelligence%20%26%20Robotics%20%282022-2023%29/Deep%20Learning/Progetto/main_%285%29.ipynb#X45sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m                        callbacks\u001b[39m=\u001b[39m[TQDMProgressBar(), EarlyStopping(monitor\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrain_loss\u001b[39m\u001b[39m\"\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/espda/OneDrive/Desktop/Universit%C3%A0/Magistrale%20-%20Artificial%20Intelligence%20%26%20Robotics%20%282022-2023%29/Deep%20Learning/Progetto/main_%285%29.ipynb#X45sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m                        )\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/espda/OneDrive/Desktop/Universit%C3%A0/Magistrale%20-%20Artificial%20Intelligence%20%26%20Robotics%20%282022-2023%29/Deep%20Learning/Progetto/main_%285%29.ipynb#X45sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model, data_module)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/espda/OneDrive/Desktop/Universit%C3%A0/Magistrale%20-%20Artificial%20Intelligence%20%26%20Robotics%20%282022-2023%29/Deep%20Learning/Progetto/main_%285%29.ipynb#X45sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# Testing\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/espda/OneDrive/Desktop/Universit%C3%A0/Magistrale%20-%20Artificial%20Intelligence%20%26%20Robotics%20%282022-2023%29/Deep%20Learning/Progetto/main_%285%29.ipynb#X45sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m test_results \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mtest(model, data_module)\n",
            "File \u001b[1;32mc:\\Users\\espda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:532\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[0;32m    531\u001b[0m _verify_strategy_supports_compile(model, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy)\n\u001b[1;32m--> 532\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[0;32m    533\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[0;32m    534\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\espda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:43\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     42\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m---> 43\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     45\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     46\u001b[0m     _call_teardown_hook(trainer)\n",
            "File \u001b[1;32mc:\\Users\\espda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:571\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mattach_data(\n\u001b[0;32m    562\u001b[0m     model, train_dataloaders\u001b[39m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[39m=\u001b[39mval_dataloaders, datamodule\u001b[39m=\u001b[39mdatamodule\n\u001b[0;32m    563\u001b[0m )\n\u001b[0;32m    565\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    566\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[0;32m    567\u001b[0m     ckpt_path,\n\u001b[0;32m    568\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    569\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    570\u001b[0m )\n\u001b[1;32m--> 571\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[0;32m    573\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[0;32m    574\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\espda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:980\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    975\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[0;32m    977\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    978\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    979\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 980\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[0;32m    982\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    983\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    984\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    985\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\espda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1023\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1021\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_sanity_check()\n\u001b[0;32m   1022\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> 1023\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[0;32m   1024\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1025\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnexpected state \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\espda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:202\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start()\n\u001b[1;32m--> 202\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance()\n\u001b[0;32m    203\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[0;32m    204\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\espda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:355\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_fetcher\u001b[39m.\u001b[39msetup(combined_loader)\n\u001b[0;32m    354\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_epoch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 355\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher)\n",
            "File \u001b[1;32mc:\\Users\\espda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:133\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdone:\n\u001b[0;32m    132\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 133\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(data_fetcher)\n\u001b[0;32m    134\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[0;32m    135\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\espda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:221\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    219\u001b[0m             batch_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mautomatic_optimization\u001b[39m.\u001b[39mrun(trainer\u001b[39m.\u001b[39moptimizers[\u001b[39m0\u001b[39m], kwargs)\n\u001b[0;32m    220\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 221\u001b[0m             batch_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmanual_optimization\u001b[39m.\u001b[39;49mrun(kwargs)\n\u001b[0;32m    223\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n\u001b[0;32m    225\u001b[0m \u001b[39m# update non-plateau LR schedulers\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \u001b[39m# update epoch-interval ones only when we are at the end of training epoch\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\espda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\manual.py:91\u001b[0m, in \u001b[0;36m_ManualOptimization.run\u001b[1;34m(self, kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_run_start()\n\u001b[0;32m     90\u001b[0m \u001b[39mwith\u001b[39;00m suppress(\u001b[39mStopIteration\u001b[39;00m):  \u001b[39m# no loop to break at this level\u001b[39;00m\n\u001b[1;32m---> 91\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(kwargs)\n\u001b[0;32m     92\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_run_end()\n",
            "File \u001b[1;32mc:\\Users\\espda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\manual.py:111\u001b[0m, in \u001b[0;36m_ManualOptimization.advance\u001b[1;34m(self, kwargs)\u001b[0m\n\u001b[0;32m    108\u001b[0m trainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\n\u001b[0;32m    110\u001b[0m \u001b[39m# manually capture logged metrics\u001b[39;00m\n\u001b[1;32m--> 111\u001b[0m training_step_output \u001b[39m=\u001b[39m call\u001b[39m.\u001b[39;49m_call_strategy_hook(trainer, \u001b[39m\"\u001b[39;49m\u001b[39mtraining_step\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[0;32m    112\u001b[0m \u001b[39mdel\u001b[39;00m kwargs  \u001b[39m# release the batch from memory\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mpost_training_step()\n",
            "File \u001b[1;32mc:\\Users\\espda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:293\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[1;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    290\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 293\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    295\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    296\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
            "File \u001b[1;32mc:\\Users\\espda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:380\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mtrain_step_context():\n\u001b[0;32m    379\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, TrainingStep)\n\u001b[1;32m--> 380\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtraining_step(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "\u001b[1;32mc:\\Users\\espda\\OneDrive\\Desktop\\Università\\Magistrale - Artificial Intelligence & Robotics (2022-2023)\\Deep Learning\\Progetto\\main_(5).ipynb Cell 34\u001b[0m in \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/espda/OneDrive/Desktop/Universit%C3%A0/Magistrale%20-%20Artificial%20Intelligence%20%26%20Robotics%20%282022-2023%29/Deep%20Learning/Progetto/main_%285%29.ipynb#X45sZmlsZQ%3D%3D?line=164'>165</a>\u001b[0m fake \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(real_images\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m1\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/espda/OneDrive/Desktop/Universit%C3%A0/Magistrale%20-%20Artificial%20Intelligence%20%26%20Robotics%20%282022-2023%29/Deep%20Learning/Progetto/main_%285%29.ipynb#X45sZmlsZQ%3D%3D?line=166'>167</a>\u001b[0m z \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(real_images\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/espda/OneDrive/Desktop/Universit%C3%A0/Magistrale%20-%20Artificial%20Intelligence%20%26%20Robotics%20%282022-2023%29/Deep%20Learning/Progetto/main_%285%29.ipynb#X45sZmlsZQ%3D%3D?line=167'>168</a>\u001b[0m gen_images \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerator(z)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/espda/OneDrive/Desktop/Universit%C3%A0/Magistrale%20-%20Artificial%20Intelligence%20%26%20Robotics%20%282022-2023%29/Deep%20Learning/Progetto/main_%285%29.ipynb#X45sZmlsZQ%3D%3D?line=169'>170</a>\u001b[0m \u001b[39m# Manually access optimizers\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/espda/OneDrive/Desktop/Universit%C3%A0/Magistrale%20-%20Artificial%20Intelligence%20%26%20Robotics%20%282022-2023%29/Deep%20Learning/Progetto/main_%285%29.ipynb#X45sZmlsZQ%3D%3D?line=170'>171</a>\u001b[0m opt_gen, opt_disc_normal, opt_disc_anomaly \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizers()\n",
            "File \u001b[1;32mc:\\Users\\espda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "\u001b[1;32mc:\\Users\\espda\\OneDrive\\Desktop\\Università\\Magistrale - Artificial Intelligence & Robotics (2022-2023)\\Deep Learning\\Progetto\\main_(5).ipynb Cell 34\u001b[0m in \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/espda/OneDrive/Desktop/Universit%C3%A0/Magistrale%20-%20Artificial%20Intelligence%20%26%20Robotics%20%282022-2023%29/Deep%20Learning/Progetto/main_%285%29.ipynb#X45sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/espda/OneDrive/Desktop/Universit%C3%A0/Magistrale%20-%20Artificial%20Intelligence%20%26%20Robotics%20%282022-2023%29/Deep%20Learning/Progetto/main_%285%29.ipynb#X45sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m     \u001b[39m# Encoder\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/espda/OneDrive/Desktop/Universit%C3%A0/Magistrale%20-%20Artificial%20Intelligence%20%26%20Robotics%20%282022-2023%29/Deep%20Learning/Progetto/main_%285%29.ipynb#X45sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m     enc1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menc_conv1(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/espda/OneDrive/Desktop/Universit%C3%A0/Magistrale%20-%20Artificial%20Intelligence%20%26%20Robotics%20%282022-2023%29/Deep%20Learning/Progetto/main_%285%29.ipynb#X45sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m     \u001b[39m#enc2 = self.enc_conv2(F.max_pool2d(enc1, kernel_size=2, stride=2))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/espda/OneDrive/Desktop/Universit%C3%A0/Magistrale%20-%20Artificial%20Intelligence%20%26%20Robotics%20%282022-2023%29/Deep%20Learning/Progetto/main_%285%29.ipynb#X45sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m     \u001b[39m#enc3 = self.enc_conv3(F.max_pool2d(enc2, kernel_size=2, stride=2))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/espda/OneDrive/Desktop/Universit%C3%A0/Magistrale%20-%20Artificial%20Intelligence%20%26%20Robotics%20%282022-2023%29/Deep%20Learning/Progetto/main_%285%29.ipynb#X45sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m     \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/espda/OneDrive/Desktop/Universit%C3%A0/Magistrale%20-%20Artificial%20Intelligence%20%26%20Robotics%20%282022-2023%29/Deep%20Learning/Progetto/main_%285%29.ipynb#X45sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m     \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/espda/OneDrive/Desktop/Universit%C3%A0/Magistrale%20-%20Artificial%20Intelligence%20%26%20Robotics%20%282022-2023%29/Deep%20Learning/Progetto/main_%285%29.ipynb#X45sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m     \u001b[39m#output = self.final_conv(dec2)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/espda/OneDrive/Desktop/Universit%C3%A0/Magistrale%20-%20Artificial%20Intelligence%20%26%20Robotics%20%282022-2023%29/Deep%20Learning/Progetto/main_%285%29.ipynb#X45sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinal_conv(enc1)\n",
            "File \u001b[1;32mc:\\Users\\espda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\espda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\espda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\espda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
            "File \u001b[1;32mc:\\Users\\espda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Calculated padded input size per channel: (3 x 3). Kernel size: (4 x 4). Kernel size can't be greater than actual input size"
          ]
        }
      ],
      "source": [
        "#Hyperparameters\n",
        "BATCH_SIZE = 10\n",
        "epochs = 1\n",
        "subsampling = True\n",
        "subsamples = 10\n",
        "accelerator_enabled = True\n",
        "\n",
        "data_module = AnomalyDetectionDataModule(batch_size=BATCH_SIZE, \n",
        "                                         subsampling=subsampling, \n",
        "                                         subsamples=subsamples)\n",
        "model = GAN()\n",
        "#logger = TensorBoardLogger(\"tb_logs\", name=\"net\",log_graph=True )\n",
        "\n",
        "\n",
        "if torch.cuda.is_available() and accelerator_enabled: \n",
        "  print(\"GPU\")\n",
        "  trainer = pl.Trainer(accelerator=\"cuda\",\n",
        "                       max_epochs=epochs,\n",
        "                       callbacks=[TQDMProgressBar(), EarlyStopping(monitor=\"train_loss\", patience=3)]\n",
        "                       )\n",
        "else : \n",
        "  print(\"CPU\")\n",
        "  trainer = pl.Trainer(accelerator=\"cpu\",\n",
        "                       max_epochs=epochs, \n",
        "                       callbacks=[TQDMProgressBar(), EarlyStopping(monitor=\"train_loss\", patience=3)]\n",
        "                       )\n",
        "\n",
        "\n",
        "trainer.fit(model, data_module)\n",
        "\n",
        "# Testing\n",
        "test_results = trainer.test(model, data_module)\n",
        "print(test_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnVw4nWYhjPK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "def assign_device(gpu):\n",
        "    if (torch.cuda.is_available() and gpu==True):\n",
        "        device = \"cuda\"\n",
        "        print(\"Cuda enabled: using GPU\")\n",
        "    else:\n",
        "        device = \"cpu\"\n",
        "        print(\"Cuda not available: using CPU\")\n",
        "    return device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cuda enabled: using GPU\n",
            "PyTorch version: 2.0.1+cu118\n",
            "Python version: 3.10.8\n"
          ]
        }
      ],
      "source": [
        "import platform\n",
        "device = assign_device(True)\n",
        "#print(\"Lightning version:\", pl.__version__)\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"Python version:\", platform.python_version())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "M_laAV6P4ayP",
        "kG1iyc556-nd",
        "xOQBdegoDIV4"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "orig_nbformat": 4,
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "014f81a6caac4aac970ba5315c042d3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1306bcb68dfe45d6bd899b8473730d8b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28bf6b7fca784dc19e69c1c93d82fa8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2b84db9a5d6c4031bc16deb65b357a95": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "47d3a3c58f474cb6bdc3f1a47cf383bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4c370e5a291f46f290a04fb1b899dfba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "519f2e73d9ab45a882ac871553ebf7fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "529290062b26453eb72028d3cef9af95": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf8a684590324c75b9903c312c87f2c9",
            "placeholder": "​",
            "style": "IPY_MODEL_28bf6b7fca784dc19e69c1c93d82fa8f",
            "value": "Sanity Checking DataLoader 0:   0%"
          }
        },
        "6dbde2d93c4e4c599ef3f5a91fe762b6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75b27c66405d4732b5d1f57f4e6051b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a579aa90896140c39a818d7092067bcd",
            "placeholder": "​",
            "style": "IPY_MODEL_d1e952e3caa44990afbf5580390cc182",
            "value": " 0/2 [00:00&lt;?, ?it/s]"
          }
        },
        "7b4d9902e0524e82b4159e56062a5565": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "8ac27229ec2142e2a72ab7a10fc86a65": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b1037bcb95949abb5bc5dfaba96331e",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_519f2e73d9ab45a882ac871553ebf7fa",
            "value": 0
          }
        },
        "8b1037bcb95949abb5bc5dfaba96331e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a579aa90896140c39a818d7092067bcd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8f1d7ef74b949fda2016983cb9e0a4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1306bcb68dfe45d6bd899b8473730d8b",
            "placeholder": "​",
            "style": "IPY_MODEL_f1c98dad392643c8936617d87a75aee9",
            "value": "Sanity Checking DataLoader 0:   0%"
          }
        },
        "c5a7ff9ae2db49b681a8c4a8e89a2fc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_529290062b26453eb72028d3cef9af95",
              "IPY_MODEL_c8c85607e00d463ebf8f3bcb66443dfb",
              "IPY_MODEL_fa12ff57f0724c9eb451b8075811be2c"
            ],
            "layout": "IPY_MODEL_2b84db9a5d6c4031bc16deb65b357a95"
          }
        },
        "c8c85607e00d463ebf8f3bcb66443dfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6dbde2d93c4e4c599ef3f5a91fe762b6",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_014f81a6caac4aac970ba5315c042d3a",
            "value": 0
          }
        },
        "cf8a684590324c75b9903c312c87f2c9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d127d6862c0e4437bbe8e1d1c89c053b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a8f1d7ef74b949fda2016983cb9e0a4a",
              "IPY_MODEL_8ac27229ec2142e2a72ab7a10fc86a65",
              "IPY_MODEL_75b27c66405d4732b5d1f57f4e6051b2"
            ],
            "layout": "IPY_MODEL_7b4d9902e0524e82b4159e56062a5565"
          }
        },
        "d1e952e3caa44990afbf5580390cc182": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f1c98dad392643c8936617d87a75aee9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fa12ff57f0724c9eb451b8075811be2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c370e5a291f46f290a04fb1b899dfba",
            "placeholder": "​",
            "style": "IPY_MODEL_47d3a3c58f474cb6bdc3f1a47cf383bd",
            "value": " 0/2 [00:00&lt;?, ?it/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
